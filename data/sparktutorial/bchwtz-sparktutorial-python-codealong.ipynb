{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d612b0a-5569-4537-a71a-d953a2b2a74f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32114e7-012a-40fd-977c-d0834be0a455",
   "metadata": {},
   "source": [
    "If you have downloaded this file manually, please ensure that everything is up to date. The easiest way to do so ist by simply cloning the corresponding repository. If you have just done so, please simply ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781fb5e-0ed3-4774-9738-33fb28ece574",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/bchwtz/bchwtz-bdsm.git\n",
    "cd ./bchwtz-bdsm/data/sparktutorial\n",
    "echo \"=== Tutorial Files ===\"\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987e768-8d45-42d1-ae26-06111ffd5537",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fd1c0-db30-47b1-bbe2-0b1cbd97a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark-3.5.0-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336da119-4cb3-49a9-a052-5b88a7c9dbcc",
   "metadata": {},
   "source": [
    "## Interacting with the System \n",
    "\n",
    "SparkContext was the preferred entry point for interacting with the system.It allows connecting to the cluster and enables a user to create RDDs, accumulators, and broadcast variables, as well as access system services. SparkContext also enables access to SQLContext and HiveContext, which provide additional functionality for working with structured and semi-structured data.\n",
    "\n",
    "SparkSession was introduced in Spark 2.0 and quickly became the preferred entry point for programming with DataFrames and Datasets, which are higher-level abstractions than RDDs. SparkSession internally creates a SparkContext object, which can be accessed through the sparkContext attribute. Therefore, you can still use SparkContext methods and features through SparkSession. SparkSession also provides a unified interface to access various data sources and formats, such as Parquet, ORC, JSON, CSV, JDBC, and Hive. SparkSession also integrates with popular Spark libraries, such as Spark Streaming or MLlib. In the following we mainly use the SparkSession interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7995be4-ef16-4903-8bb3-ff83d8c7ed2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a441d-14fa-414d-894b-b38f88f0173c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187f36d-5c2f-4c70-9f8a-49308773b113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2bf6403-970c-4989-a246-b59af869b47d",
   "metadata": {},
   "source": [
    "# Closing the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf7ecd-0b7d-4a58-8f27-557290cb7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
