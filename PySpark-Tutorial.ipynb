{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4987e768-8d45-42d1-ae26-06111ffd5537",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fd1c0-db30-47b1-bbe2-0b1cbd97a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark-3.5.0-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336da119-4cb3-49a9-a052-5b88a7c9dbcc",
   "metadata": {},
   "source": [
    "## Interacting with the System \n",
    "\n",
    "SparkContext was the preferred entry point for interacting with the system.It allows connecting to the cluster and enables a user to create RDDs, accumulators, and broadcast variables, as well as access system services. SparkContext also enables access to SQLContext and HiveContext, which provide additional functionality for working with structured and semi-structured data.\n",
    "\n",
    "SparkSession was introduced in Spark 2.0 and quickly became the preferred entry point for programming with DataFrames and Datasets, which are higher-level abstractions than RDDs. SparkSession internally creates a SparkContext object, which can be accessed through the sparkContext attribute. Therefore, you can still use SparkContext methods and features through SparkSession. SparkSession also provides a unified interface to access various data sources and formats, such as Parquet, ORC, JSON, CSV, JDBC, and Hive. SparkSession also integrates with popular Spark libraries, such as Spark Streaming or MLlib. In the following we mainly use the SparkSession interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53020b-1e79-4893-a13a-4968fa120fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "ss = SparkSession.builder \\\n",
    "    .appName(\"bchwtz-bdda-session\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a38399-27a1-4d8f-9d1d-163df9b193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly access the SparkContext\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddd5ec-672f-4a9a-a878-68a12305c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c183f-d812-4707-8474-3a44fe63dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18233452-c721-4acd-8e98-d270a4d38a10",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets ([Doku](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html))\n",
    "\n",
    "Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged even though the RDD API is not deprecated. The RDD technology still underlies the Dataset API.\n",
    "\n",
    "Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8098525-7f2c-43ba-853d-e94f892aeac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a first RDD\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "numbers_rdd = ss.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0607173-2e93-4539-bbe0-5addc08b89c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "numbers_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186167d-8046-4216-8c11-81a5edaea661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of tuples\n",
    "data = [(\"Homer\", 38), (\"Marge\", 32), (\"Bart\", 12)]\n",
    "data_rdd = ss.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52c911-307b-49d1-9597-2a72a6fa6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde3fe1-fc5e-4444-8d2f-0fd065a6f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90d5cc-d42f-44c8-acea-245d04fe989b",
   "metadata": {},
   "source": [
    "## Actions (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205afe3-bec4-430e-892e-588dc94b0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count action: Count the number of elements in the RDD\n",
    "data_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31424104-5c98-46cb-8d38-d2d746098728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First action: Retrieve the first element of the RDD\n",
    "data_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cd789-145b-445f-80f2-e50d780952d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take action: Retrieve the n elements of the RDD\n",
    "data_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f211f-4856-424a-b846-dd438eea5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach action: Print each element of the RDD\n",
    "def f(x): print(x)\n",
    "data_rdd.foreach(f) # Q: Where is the output? A:In StdOut of the worker not in the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ee7fd-c914-4819-bce7-b57666397fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each element of the RDD in the driver\n",
    "for w in data_rdd.toLocalIterator():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ceea7e-40a8-4de4-babb-a791a8a941fe",
   "metadata": {},
   "source": [
    "## Transformations (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c1a21-5d99-4ead-ba0a-9590a1326f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = data_rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbb94c-5ec9-4f40-933a-99fff32568f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SortBy transformation: Sort the RDD by age in ascending order\n",
    "sorted_rdd = data_rdd.sortBy(lambda x: x[1], ascending=True)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca448927-746f-4842-b7aa-1db83fbc2611",
   "metadata": {},
   "source": [
    "## I/O (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d9294-a1dd-4121-b45a-a5430257a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save action: Save the RDD to a text file\n",
    "data_rdd.saveAsTextFile(\"simpsons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70605d9-e253-4841-a7a6-ad777e77df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read action: Create RDD from text file\n",
    "imported_rdd = ss.sparkContext.textFile(\"simpsons\")\n",
    "imported_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3aefb-7880-4673-bdcd-a3c21c663ced",
   "metadata": {},
   "source": [
    "## Obligatory Word Count (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90de336-de76-4b2c-90ae-36aa1d8b859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RDD\n",
    "rdd = ss.sparkContext.textFile(\"./data/data.txt\")\n",
    "result_rdd = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "result_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bb2a2-994a-4a83-8312-f3bf09917ba7",
   "metadata": {},
   "source": [
    "# DataFrames (DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8195a4-26e0-437c-9a2b-004c4ae6f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "head -10 ./data/products.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebded5-2d2e-4cc3-bc21-e39be9c7f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "df = ss.read.csv(\"./data/products.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924f49f-545c-43e8-859f-fabd5e494c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779ec4d-a1cb-48b1-8cc7-58c727ee9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50668c-65cd-4af3-b2b8-ed75e7e99283",
   "metadata": {},
   "source": [
    "## CSV / Read Data: Define Schema (DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfaa79-8c0d-4b34-9ce3-425797c27f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a613f-ab28-486a-b6f0-4e04f471a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"category\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"quantity\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"price\", dataType=DoubleType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcfc7df-41a3-40c7-a24d-bdc88f6d1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with schema definition\n",
    "df = ss.read.csv(\"./data/products.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01eb7b1-89df-43dc-adb3-b0d08785f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e76e16-a663-4dbd-b67e-4bcd4a29c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953f619-84f5-4428-b631-9c34a9cb9c6f",
   "metadata": {},
   "source": [
    "## CSV / Read Data: Infer a schema (DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8efbb8-1f9b-4c41-a571-ad714ae01ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with inferSchema\n",
    "df = ss.read.csv(\"./data/products.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9feb3d-5670-4557-8eed-9b2a48b3b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4edd1-04f6-493f-a1d6-22f3f8b0b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f075f71-9087-4bcb-b413-06d4c1ce49ae",
   "metadata": {},
   "source": [
    "## JSON / Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1999e36-5762-4f31-a539-be8eb38bfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -10 data/products_singleline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4f7dc-5c06-4def-ac1a-de4f935775c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON (Single Line)\n",
    "df = ss.read.json(\"./data/products_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b876aca-1d9b-4c7b-8250-9157dd27d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -10 data/products_multiline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e3a6c-8adb-4bd3-a928-f5f6e581bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON (Multi Line)\n",
    "df = ss.read.json(\"./data/products_multiline.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c90f50-a7e8-41ed-962a-fa84d594cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913775a1-53a8-4a34-9976-4952eec66b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364f2af-e0a6-4cf3-9b58-0047ec96fecb",
   "metadata": {},
   "source": [
    "## Parquet / Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cb722-ecde-47ef-976c-90afa3016fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe into parquet file\n",
    "df.write.parquet(\"./data/products.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f028b15-4240-47b0-bea7-dd6c7fbd6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273914d2-9fa3-4540-80c8-63bde780424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54463f31-0b09-40d1-a310-53a7ac66d215",
   "metadata": {},
   "source": [
    "## Working with DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f927e2-593c-43ae-8d1d-b5d89acb0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -10 data/stocks.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f859a1-249c-4089-8d4b-a79cc84a954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "df = ss.read.csv(\"./data/stocks.txt\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7927704-fea3-4633-8ef8-a53bb8f01f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975969c-5f41-490d-8d98-5fd05f536fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the initial DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c916b51-4240-4b37-a5bb-fcaf115b37d1",
   "metadata": {},
   "source": [
    "## Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f3716-0b45-47ef-94b6-ba3bbbf39481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "selected_columns = df.select(\"id\", \"name\", \"price\")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500d27e-e5bf-4cfb-a1af-79603fa5ae39",
   "metadata": {},
   "source": [
    "## Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a4aa7-6313-42b3-9800-2ab90d3282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_data = df.filter(df.quantity > 20)\n",
    "filtered_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1dcae2-3657-439c-b4dc-dd1ea002dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea73ec-1c5d-4d96-b356-3752a0531c25",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08fe917-0019-4e0c-8179-ac44b7cab587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with another DataFrame\n",
    "df2 = df.select(\"id\", \"category\").limit(10)\n",
    "joined_data = df.join(df2, \"id\", \"inner\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96ea35-b44b-4aeb-969a-23a0cb8761a6",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ebbf0-b378-4637-a9a8-0775f2b3fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a column\n",
    "sorted_data = df.orderBy(\"price\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce6a8b-5b91-4e60-b6bc-aea22407ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a column desc\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_data = df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7ab94-a519-4c81-bd42-c79576ba4e27",
   "metadata": {},
   "source": [
    "## Unique Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7f25b-9d7f-4e91-a3e1-77f1d7585fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct product category\n",
    "distinct_rows = df.select(\"category\").distinct()\n",
    "print(\"Distinct Product Categories:\")\n",
    "distinct_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a8a1b-0ab2-4771-9429-d5e3e4c89d16",
   "metadata": {},
   "source": [
    "## Remove Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e857b-1596-4ad5-a8ca-029c8437d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "dropped_columns = df.drop(\"quantity\", \"category\")\n",
    "dropped_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6c39d-34d4-4246-a118-149c6e1506d1",
   "metadata": {},
   "source": [
    "## Add Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca03e7-2907-4a3f-a0d6-b0c6f0ad9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new calculated column\n",
    "df_with_new_column = df.withColumn(\"revenue\", df.quantity * df.price)\n",
    "df_with_new_column.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17ca52-0742-4ddb-93db-8205c1905415",
   "metadata": {},
   "source": [
    "## Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c7d25-a0ae-4513-89cf-732f5b128283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns using alias\n",
    "df_with_alias = df.withColumnRenamed(\"price\", \"product_price\")\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d3529-790f-4f23-b132-441acbbb05c7",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b234b83-6801-40ae-9e7e-9d306c719618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Linear Regression with PySpark MLlib\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172973c-9dc0-418f-9b22-dff4269622d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "data = spark.read.csv(SparkFiles.get(\"BostonHousing.csv\"), header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dda1b2-c65d-4847-a2fa-07feb0d4e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"crim\", \"zn\", \"indus\", \"chas\", \"nox\", \"rm\", \"age\", \"dis\", \"rad\", \"tax\", \"ptratio\", \"b\", \"lstat\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "data = assembler.transform(data)\n",
    "final_data = data.select(\"features\", \"medv\")\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd983632-fe77-4e49-8a5d-46f324bc22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"medv\", predictionCol=\"predicted_medv\")\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e91001-3bc6-4af6-b569-ff42d9f8a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"predicted_medv\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"predicted_medv\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared (R2) on test data: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322238e4-4398-4fe1-a834-d0c257c4a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = lr_model.coefficients\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "print(\"Coefficients: \", coefficients)\n",
    "print(\"Intercept: {:.3f}\".format(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85422b39-efa1-4277-ae9f-a5e2f015c4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82b495-c425-446d-8c8a-bf4af4b973d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "lr_model.save(\"lr_model\")\n",
    "\n",
    "# Load the model\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "loaded_model = LinearRegressionModel.load(\"lr_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf6403-970c-4989-a246-b59af869b47d",
   "metadata": {},
   "source": [
    "# Closing the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf7ecd-0b7d-4a58-8f27-557290cb7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
